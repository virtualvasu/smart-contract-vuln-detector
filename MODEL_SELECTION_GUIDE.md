# Model Selection Guide for Streamlit App

## üìã Overview

The Streamlit app now supports **multiple AI models** for vulnerability detection. Users can select their preferred model based on their needs for accuracy, speed, or balance.

---

## üéØ Available Models

### 1. **Ensemble - Stacking (Best)** ü•á **RECOMMENDED**
- **Type**: Neural Ensemble combining CodeBERT, LSTM, and CNN
- **F1-Score**: 53.7%
- **Accuracy**: 92.8%
- **Precision**: 45.3%
- **Recall**: 66.0%
- **AUC**: 0.903

**Best For:**
- ‚úÖ Production deployments requiring highest accuracy
- ‚úÖ Critical contract audits
- ‚úÖ When false negatives must be minimized
- ‚úÖ Comprehensive vulnerability detection

**Tradeoffs:**
- ‚è±Ô∏è Slower inference (processes through 3 models + ensemble layer)
- üíæ Higher memory usage

---

### 2. **Ensemble - Attention** ü•à
- **Type**: Attention-weighted ensemble
- **F1-Score**: 49.9%
- **Accuracy**: 91.1%
- **Precision**: 38.7%
- **Recall**: 70.3%
- **AUC**: 0.904

**Best For:**
- ‚úÖ High recall scenarios (catching most vulnerabilities)
- ‚úÖ Exploratory analysis
- ‚úÖ When you want to review more potential issues

**Tradeoffs:**
- ‚ö†Ô∏è Higher false positive rate
- ‚è±Ô∏è Similar speed to stacking ensemble

---

### 3. **CodeBERT (Latest)**
- **Type**: Transformer-based model for code
- **F1-Score**: 61.8%
- **Accuracy**: 96.2%
- **Precision**: 86.4%
- **Recall**: 48.1%
- **AUC**: 0.858

**Best For:**
- ‚úÖ Balanced performance
- ‚úÖ When precision is important (fewer false alarms)
- ‚úÖ Good general-purpose choice
- ‚úÖ Understanding code context and semantics

**Tradeoffs:**
- ‚ö†Ô∏è May miss some vulnerabilities (lower recall)
- ‚è±Ô∏è Moderate inference speed

---

### 4. **LSTM (Latest)**
- **Type**: Recurrent Neural Network
- **F1-Score**: 45.7%
- **Accuracy**: 94.9%
- **Precision**: 71.7%
- **Recall**: 33.5%
- **AUC**: 0.891

**Best For:**
- ‚úÖ Fast inference
- ‚úÖ Sequential pattern detection
- ‚úÖ Resource-constrained environments
- ‚úÖ High precision needs

**Tradeoffs:**
- ‚ö†Ô∏è Lower recall (misses more vulnerabilities)
- ‚ö° Faster than transformer models

---

### 5. **CNN (Latest)**
- **Type**: Convolutional Neural Network
- **F1-Score**: 33.7%
- **Accuracy**: 78.7%
- **Precision**: 21.0%
- **Recall**: 85.4%
- **AUC**: 0.905

**Best For:**
- ‚úÖ Detecting local code patterns
- ‚úÖ Very high recall (catches almost everything)
- ‚úÖ Quick scanning
- ‚úÖ Baseline comparison

**Tradeoffs:**
- ‚ö†Ô∏è High false positive rate
- ‚ö†Ô∏è Lower overall accuracy

---

## üìä Model Comparison

| Model | F1-Score | Accuracy | Precision | Recall | Speed | Memory |
|-------|----------|----------|-----------|--------|-------|--------|
| **Ensemble Stacking** ü•á | 53.7% | 92.8% | 45.3% | 66.0% | Slow | High |
| Ensemble Attention | 49.9% | 91.1% | 38.7% | 70.3% | Slow | High |
| **CodeBERT** | 61.8% | 96.2% | 86.4% | 48.1% | Medium | Medium |
| LSTM | 45.7% | 94.9% | 71.7% | 33.5% | Fast | Low |
| CNN | 33.7% | 78.7% | 21.0% | 85.4% | Fast | Low |

---

## üí° Selection Guide

### Choose **Ensemble - Stacking** if:
- You need the best overall performance
- Accuracy is critical
- You're auditing important contracts
- Resource usage is not a concern

### Choose **CodeBERT** if:
- You want balanced performance
- You prefer fewer false alarms (high precision)
- You need moderate inference speed
- You want good semantic understanding

### Choose **LSTM** if:
- Speed is important
- You have limited resources
- You prefer high precision over recall
- Sequential patterns matter

### Choose **CNN** if:
- You want to catch as many vulnerabilities as possible
- You can manually review many results
- You need very fast inference
- Local code patterns are important

---

## üöÄ Usage in Streamlit

1. **Launch the app**:
   ```bash
   ./run_streamlit.sh
   # or
   streamlit run streamlit_app.py
   ```

2. **Select Model**:
   - Look at the sidebar on the left
   - Find the "ü§ñ Model Selection" section
   - Choose your preferred model from the dropdown
   - The app will automatically load it

3. **View Model Info**:
   - Expand the "‚ÑπÔ∏è Model Information" section
   - See detailed explanations of each model type
   - Understand the metrics

4. **Analyze Contracts**:
   - Paste your Solidity contract code
   - Click "üîç Analyze Vulnerabilities"
   - Review the results with the selected model

---

## üîß Technical Details

### Model Architectures

**CodeBERT**:
- Pre-trained transformer model from Microsoft
- Fine-tuned on vulnerability detection
- Uses attention mechanisms for code understanding

**LSTM**:
- Bidirectional LSTM with 2 layers
- Embedding dimension: 128
- Hidden dimension: 256
- Dropout: 0.3

**CNN**:
- Multiple convolutional filters (3, 4, 5-gram)
- 100 filters per size
- Max pooling for feature extraction
- Dropout: 0.5

**Ensemble**:
- Combines predictions from all three models
- Stacking: Neural network meta-learner
- Attention: Weighted combination based on learned attention

---

## üìà Performance Metrics Explained

- **F1-Score**: Harmonic mean of precision and recall (0-100%, higher is better)
- **Accuracy**: Percentage of correct predictions (0-100%, higher is better)
- **Precision**: Of flagged vulnerabilities, how many are real (0-100%, higher is better)
- **Recall**: Of real vulnerabilities, how many are detected (0-100%, higher is better)
- **AUC**: Area Under ROC Curve (0-1, higher is better)

---

## üéì Best Practices

1. **For Production Audits**: Use Ensemble - Stacking
2. **For Development**: Use CodeBERT for quick checks
3. **For Research**: Try multiple models and compare results
4. **For Speed**: Use LSTM or CNN
5. **For Maximum Coverage**: Use CNN or Ensemble - Attention

---

## üêõ Troubleshooting

**Model not loading?**
- Check that model files exist in `models/` or `results/checkpoints/`
- Ensure models were trained successfully
- Check console for error messages

**Slow performance?**
- Try LSTM or CNN for faster inference
- Check if running on GPU (CUDA available)
- Reduce contract size or analyze fewer functions

**Unexpected results?**
- Try different models and compare
- Check model metrics for that specific model
- Review the contract code for edge cases

---

## üìö Related Files

- `streamlit_app.py`: Main application file
- `results/metrics/ensemble_comparison_*.json`: Ensemble performance metrics
- `results/metrics/training_summary_*.json`: Individual model metrics
- `results/benchmark/benchmark_report.md`: Comparison with other tools

---

**Last Updated**: November 16, 2025
**Version**: 2.0 (Multi-Model Support)
