{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12f7e817",
   "metadata": {},
   "source": [
    "# Smart Contract Vulnerability Detection Dataset Analysis\n",
    "\n",
    "This notebook provides a comprehensive analysis of the smart contract vulnerability detection dataset. The dataset contains various types of vulnerabilities including:\n",
    "- Overflow-Underflow\n",
    "- Re-entrancy\n",
    "- Timestamp-Dependency\n",
    "- TOD (Transaction Order Dependency)\n",
    "- tx.origin\n",
    "- Unchecked-Send\n",
    "- Unhandled-Exceptions\n",
    "\n",
    "Each vulnerability category contains Solidity smart contract files (.sol) and corresponding bug log CSV files that track vulnerability locations and characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705f8be4",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82ae3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "from collections import Counter, defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"Working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb65fa0",
   "metadata": {},
   "source": [
    "## 2. Load Dataset from Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3552d6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset paths\n",
    "dataset_path = './dataset'\n",
    "buggy_contracts_path = os.path.join(dataset_path, 'buggy_contracts')\n",
    "\n",
    "# Get all vulnerability categories\n",
    "vulnerability_types = [d for d in os.listdir(buggy_contracts_path) \n",
    "                      if os.path.isdir(os.path.join(buggy_contracts_path, d))]\n",
    "\n",
    "print(\"Vulnerability types found:\")\n",
    "for i, vuln_type in enumerate(vulnerability_types, 1):\n",
    "    print(f\"{i}. {vuln_type}\")\n",
    "\n",
    "print(f\"\\nTotal vulnerability categories: {len(vulnerability_types)}\")\n",
    "\n",
    "# Function to load all CSV bug logs\n",
    "def load_bug_logs():\n",
    "    \"\"\"Load all BugLog CSV files from all vulnerability categories\"\"\"\n",
    "    all_bug_data = []\n",
    "    \n",
    "    for vuln_type in vulnerability_types:\n",
    "        vuln_path = os.path.join(buggy_contracts_path, vuln_type)\n",
    "        csv_files = glob.glob(os.path.join(vuln_path, 'BugLog_*.csv'))\n",
    "        \n",
    "        for csv_file in csv_files:\n",
    "            try:\n",
    "                df = pd.read_csv(csv_file)\n",
    "                df['vulnerability_type'] = vuln_type\n",
    "                df['file_name'] = os.path.basename(csv_file)\n",
    "                all_bug_data.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {csv_file}: {e}\")\n",
    "    \n",
    "    return pd.concat(all_bug_data, ignore_index=True) if all_bug_data else pd.DataFrame()\n",
    "\n",
    "# Function to get contract file information\n",
    "def get_contract_info():\n",
    "    \"\"\"Get information about Solidity contract files\"\"\"\n",
    "    contract_info = []\n",
    "    \n",
    "    for vuln_type in vulnerability_types:\n",
    "        vuln_path = os.path.join(buggy_contracts_path, vuln_type)\n",
    "        sol_files = glob.glob(os.path.join(vuln_path, '*.sol'))\n",
    "        \n",
    "        for sol_file in sol_files:\n",
    "            try:\n",
    "                with open(sol_file, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    line_count = len(content.split('\\n'))\n",
    "                    char_count = len(content)\n",
    "                \n",
    "                contract_info.append({\n",
    "                    'file_path': sol_file,\n",
    "                    'file_name': os.path.basename(sol_file),\n",
    "                    'vulnerability_type': vuln_type,\n",
    "                    'line_count': line_count,\n",
    "                    'char_count': char_count,\n",
    "                    'file_size_kb': os.path.getsize(sol_file) / 1024\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {sol_file}: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(contract_info)\n",
    "\n",
    "# Load the data\n",
    "print(\"Loading bug log data...\")\n",
    "bug_logs_df = load_bug_logs()\n",
    "\n",
    "print(\"Loading contract information...\")\n",
    "contracts_df = get_contract_info()\n",
    "\n",
    "print(f\"\\nData loaded successfully!\")\n",
    "print(f\"Bug logs shape: {bug_logs_df.shape}\")\n",
    "print(f\"Contracts info shape: {contracts_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ef2c0e",
   "metadata": {},
   "source": [
    "## 3. Basic Dataset Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de4a5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== BUG LOGS DATASET ===\")\n",
    "print(f\"Shape: {bug_logs_df.shape}\")\n",
    "print(f\"Columns: {list(bug_logs_df.columns)}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(bug_logs_df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"=== CONTRACTS DATASET ===\")\n",
    "print(f\"Shape: {contracts_df.shape}\")\n",
    "print(f\"Columns: {list(contracts_df.columns)}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(contracts_df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"=== DATA TYPES ===\")\n",
    "print(\"\\nBug Logs Data Types:\")\n",
    "print(bug_logs_df.dtypes)\n",
    "print(\"\\nContracts Data Types:\")\n",
    "print(contracts_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9274f8ef",
   "metadata": {},
   "source": [
    "## 4. Dataset Shape and Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb321c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset dimensions and structure analysis\n",
    "print(\"=== DATASET DIMENSIONS ===\")\n",
    "print(f\"Bug Logs Dataset:\")\n",
    "print(f\"  - Total vulnerabilities logged: {len(bug_logs_df)}\")\n",
    "print(f\"  - Number of features: {len(bug_logs_df.columns)}\")\n",
    "\n",
    "print(f\"\\nContracts Dataset:\")\n",
    "print(f\"  - Total contracts: {len(contracts_df)}\")\n",
    "print(f\"  - Number of features: {len(contracts_df.columns)}\")\n",
    "\n",
    "# Count contracts and bug logs per vulnerability type\n",
    "vuln_counts = bug_logs_df['vulnerability_type'].value_counts()\n",
    "contract_counts = contracts_df['vulnerability_type'].value_counts()\n",
    "\n",
    "print(\"\\n=== VULNERABILITY TYPE DISTRIBUTION ===\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Vulnerability_Type': vuln_counts.index,\n",
    "    'Bug_Logs_Count': vuln_counts.values,\n",
    "    'Contract_Files_Count': contract_counts.values\n",
    "})\n",
    "print(comparison_df)\n",
    "\n",
    "print(\"\\n=== SAMPLE DATA PREVIEW ===\")\n",
    "print(\"Last 3 rows of Bug Logs:\")\n",
    "print(bug_logs_df.tail(3))\n",
    "\n",
    "print(\"\\nLast 3 rows of Contracts:\")\n",
    "print(contracts_df.tail(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48099058",
   "metadata": {},
   "source": [
    "## 5. Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0cf26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"=== MISSING VALUES ANALYSIS ===\")\n",
    "\n",
    "print(\"Bug Logs Dataset Missing Values:\")\n",
    "bug_missing = bug_logs_df.isnull().sum()\n",
    "bug_missing_pct = (bug_missing / len(bug_logs_df)) * 100\n",
    "bug_missing_summary = pd.DataFrame({\n",
    "    'Column': bug_missing.index,\n",
    "    'Missing_Count': bug_missing.values,\n",
    "    'Missing_Percentage': bug_missing_pct.values\n",
    "})\n",
    "print(bug_missing_summary)\n",
    "\n",
    "print(\"\\nContracts Dataset Missing Values:\")\n",
    "contract_missing = contracts_df.isnull().sum()\n",
    "contract_missing_pct = (contract_missing / len(contracts_df)) * 100\n",
    "contract_missing_summary = pd.DataFrame({\n",
    "    'Column': contract_missing.index,\n",
    "    'Missing_Count': contract_missing.values,\n",
    "    'Missing_Percentage': contract_missing_pct.values\n",
    "})\n",
    "print(contract_missing_summary)\n",
    "\n",
    "# Visualize missing values if any exist\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Bug logs missing values heatmap\n",
    "if bug_logs_df.isnull().sum().sum() > 0:\n",
    "    sns.heatmap(bug_logs_df.isnull(), ax=axes[0], cbar=True, yticklabels=False)\n",
    "    axes[0].set_title('Missing Values in Bug Logs Dataset')\n",
    "else:\n",
    "    axes[0].text(0.5, 0.5, 'No Missing Values', ha='center', va='center', transform=axes[0].transAxes)\n",
    "    axes[0].set_title('Missing Values in Bug Logs Dataset')\n",
    "\n",
    "# Contracts missing values heatmap\n",
    "if contracts_df.isnull().sum().sum() > 0:\n",
    "    sns.heatmap(contracts_df.isnull(), ax=axes[1], cbar=True, yticklabels=False)\n",
    "    axes[1].set_title('Missing Values in Contracts Dataset')\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, 'No Missing Values', ha='center', va='center', transform=axes[1].transAxes)\n",
    "    axes[1].set_title('Missing Values in Contracts Dataset')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cc5a7d",
   "metadata": {},
   "source": [
    "## 6. Data Types and Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c27babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and memory usage analysis\n",
    "print(\"=== BUG LOGS DATASET - DATA TYPES AND MEMORY ===\")\n",
    "print(bug_logs_df.info(memory_usage='deep'))\n",
    "\n",
    "print(\"\\n=== CONTRACTS DATASET - DATA TYPES AND MEMORY ===\")\n",
    "print(contracts_df.info(memory_usage='deep'))\n",
    "\n",
    "# Analyze unique values in categorical columns\n",
    "print(\"\\n=== UNIQUE VALUES ANALYSIS ===\")\n",
    "\n",
    "print(\"Bug Logs - Categorical Columns:\")\n",
    "for col in bug_logs_df.select_dtypes(include=['object']).columns:\n",
    "    unique_count = bug_logs_df[col].nunique()\n",
    "    print(f\"  {col}: {unique_count} unique values\")\n",
    "    if unique_count <= 20:  # Show unique values if not too many\n",
    "        print(f\"    Values: {sorted(bug_logs_df[col].unique())}\")\n",
    "\n",
    "print(\"\\nContracts - Categorical Columns:\")\n",
    "for col in contracts_df.select_dtypes(include=['object']).columns:\n",
    "    unique_count = contracts_df[col].nunique()\n",
    "    print(f\"  {col}: {unique_count} unique values\")\n",
    "    if unique_count <= 20:  # Show unique values if not too many\n",
    "        print(f\"    Values: {sorted(contracts_df[col].unique())}\")\n",
    "\n",
    "# Memory optimization suggestions\n",
    "print(\"\\n=== MEMORY OPTIMIZATION OPPORTUNITIES ===\")\n",
    "for df_name, df in [(\"Bug Logs\", bug_logs_df), (\"Contracts\", contracts_df)]:\n",
    "    print(f\"\\n{df_name} Dataset:\")\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'int64':\n",
    "            col_min, col_max = df[col].min(), df[col].max()\n",
    "            if col_min >= 0 and col_max <= 255:\n",
    "                print(f\"  {col}: Can be optimized to uint8\")\n",
    "            elif col_min >= -128 and col_max <= 127:\n",
    "                print(f\"  {col}: Can be optimized to int8\")\n",
    "            elif col_min >= 0 and col_max <= 65535:\n",
    "                print(f\"  {col}: Can be optimized to uint16\")\n",
    "        elif df[col].dtype == 'float64':\n",
    "            print(f\"  {col}: Can potentially be optimized to float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79570c00",
   "metadata": {},
   "source": [
    "## 7. Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da95165d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summaries for numerical and categorical features\n",
    "print(\"=== BUG LOGS STATISTICAL SUMMARY ===\")\n",
    "print(\"\\nNumerical Features:\")\n",
    "print(bug_logs_df.describe())\n",
    "\n",
    "print(\"\\nCategorical Features:\")\n",
    "for col in bug_logs_df.select_dtypes(include=['object']).columns:\n",
    "    print(f\"\\n{col}:\")\n",
    "    value_counts = bug_logs_df[col].value_counts()\n",
    "    print(value_counts)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== CONTRACTS STATISTICAL SUMMARY ===\")\n",
    "print(\"\\nNumerical Features:\")\n",
    "print(contracts_df.describe())\n",
    "\n",
    "print(\"\\nCategorical Features:\")\n",
    "for col in contracts_df.select_dtypes(include=['object']).columns:\n",
    "    print(f\"\\n{col}:\")\n",
    "    value_counts = contracts_df[col].value_counts()\n",
    "    print(value_counts.head(10))  # Show top 10 to avoid too much output\n",
    "\n",
    "# Additional statistical insights\n",
    "print(\"\\n=== ADDITIONAL INSIGHTS ===\")\n",
    "\n",
    "print(\"\\nBug Logs Insights:\")\n",
    "if 'loc' in bug_logs_df.columns:\n",
    "    print(f\"  - Vulnerability locations range from {bug_logs_df['loc'].min()} to {bug_logs_df['loc'].max()}\")\n",
    "    print(f\"  - Average vulnerability location: {bug_logs_df['loc'].mean():.2f}\")\n",
    "\n",
    "if 'length' in bug_logs_df.columns:\n",
    "    print(f\"  - Vulnerability lengths range from {bug_logs_df['length'].min()} to {bug_logs_df['length'].max()}\")\n",
    "    print(f\"  - Average vulnerability length: {bug_logs_df['length'].mean():.2f}\")\n",
    "\n",
    "print(\"\\nContract File Insights:\")\n",
    "print(f\"  - Contract sizes range from {contracts_df['line_count'].min()} to {contracts_df['line_count'].max()} lines\")\n",
    "print(f\"  - Average contract size: {contracts_df['line_count'].mean():.2f} lines\")\n",
    "print(f\"  - File sizes range from {contracts_df['file_size_kb'].min():.2f} to {contracts_df['file_size_kb'].max():.2f} KB\")\n",
    "print(f\"  - Average file size: {contracts_df['file_size_kb'].mean():.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3015c28d",
   "metadata": {},
   "source": [
    "## 8. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1b0282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Vulnerability type distribution\n",
    "vuln_counts = bug_logs_df['vulnerability_type'].value_counts()\n",
    "axes[0, 0].pie(vuln_counts.values, labels=vuln_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "axes[0, 0].set_title('Distribution of Vulnerability Types')\n",
    "\n",
    "# 2. Contract file counts by vulnerability type\n",
    "contract_counts = contracts_df['vulnerability_type'].value_counts()\n",
    "axes[0, 1].bar(range(len(contract_counts)), contract_counts.values)\n",
    "axes[0, 1].set_xticks(range(len(contract_counts)))\n",
    "axes[0, 1].set_xticklabels(contract_counts.index, rotation=45, ha='right')\n",
    "axes[0, 1].set_title('Number of Contract Files by Vulnerability Type')\n",
    "axes[0, 1].set_ylabel('Number of Files')\n",
    "\n",
    "# 3. Contract line count distribution\n",
    "axes[0, 2].hist(contracts_df['line_count'], bins=30, alpha=0.7, edgecolor='black')\n",
    "axes[0, 2].set_title('Distribution of Contract File Line Counts')\n",
    "axes[0, 2].set_xlabel('Number of Lines')\n",
    "axes[0, 2].set_ylabel('Frequency')\n",
    "\n",
    "# 4. Bug location distribution (if available)\n",
    "if 'loc' in bug_logs_df.columns:\n",
    "    axes[1, 0].hist(bug_logs_df['loc'], bins=30, alpha=0.7, color='orange', edgecolor='black')\n",
    "    axes[1, 0].set_title('Distribution of Bug Locations')\n",
    "    axes[1, 0].set_xlabel('Line Number')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# 5. Bug length distribution (if available)\n",
    "if 'length' in bug_logs_df.columns:\n",
    "    axes[1, 1].hist(bug_logs_df['length'], bins=20, alpha=0.7, color='green', edgecolor='black')\n",
    "    axes[1, 1].set_title('Distribution of Bug Lengths')\n",
    "    axes[1, 1].set_xlabel('Length (Lines)')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "# 6. File size distribution\n",
    "axes[1, 2].hist(contracts_df['file_size_kb'], bins=30, alpha=0.7, color='purple', edgecolor='black')\n",
    "axes[1, 2].set_title('Distribution of Contract File Sizes')\n",
    "axes[1, 2].set_xlabel('File Size (KB)')\n",
    "axes[1, 2].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363ad2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional detailed visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Box plot of contract line counts by vulnerability type\n",
    "contracts_df.boxplot(column='line_count', by='vulnerability_type', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Contract Line Count Distribution by Vulnerability Type')\n",
    "axes[0, 0].set_xlabel('Vulnerability Type')\n",
    "axes[0, 0].set_ylabel('Line Count')\n",
    "plt.setp(axes[0, 0].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# 2. Box plot of file sizes by vulnerability type\n",
    "contracts_df.boxplot(column='file_size_kb', by='vulnerability_type', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('File Size Distribution by Vulnerability Type')\n",
    "axes[0, 1].set_xlabel('Vulnerability Type')\n",
    "axes[0, 1].set_ylabel('File Size (KB)')\n",
    "plt.setp(axes[0, 1].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# 3. Vulnerability count per file (if multiple vulnerabilities per file)\n",
    "if 'file_name' in bug_logs_df.columns:\n",
    "    vuln_per_file = bug_logs_df.groupby('file_name').size()\n",
    "    axes[1, 0].hist(vuln_per_file, bins=20, alpha=0.7, color='red', edgecolor='black')\n",
    "    axes[1, 0].set_title('Number of Vulnerabilities per File')\n",
    "    axes[1, 0].set_xlabel('Number of Vulnerabilities')\n",
    "    axes[1, 0].set_ylabel('Number of Files')\n",
    "\n",
    "# 4. Approach distribution (if available)\n",
    "if 'approach' in bug_logs_df.columns:\n",
    "    approach_counts = bug_logs_df['approach'].value_counts()\n",
    "    axes[1, 1].pie(approach_counts.values, labels=approach_counts.index, autopct='%1.1f%%')\n",
    "    axes[1, 1].set_title('Distribution of Bug Introduction Approaches')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de65e13",
   "metadata": {},
   "source": [
    "## 9. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0a13a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis for numerical features\n",
    "print(\"=== CORRELATION ANALYSIS ===\")\n",
    "\n",
    "# Bug logs correlation matrix\n",
    "print(\"Bug Logs Numerical Features Correlation:\")\n",
    "bug_logs_numeric = bug_logs_df.select_dtypes(include=[np.number])\n",
    "if not bug_logs_numeric.empty:\n",
    "    bug_corr = bug_logs_numeric.corr()\n",
    "    print(bug_corr)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(bug_corr, annot=True, cmap='coolwarm', center=0, \n",
    "                square=True, cbar_kws={'shrink': 0.8})\n",
    "    plt.title('Correlation Matrix - Bug Logs Numerical Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No numerical features found in bug logs dataset\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Contracts correlation matrix\n",
    "print(\"Contracts Numerical Features Correlation:\")\n",
    "contracts_numeric = contracts_df.select_dtypes(include=[np.number])\n",
    "if not contracts_numeric.empty:\n",
    "    contracts_corr = contracts_numeric.corr()\n",
    "    print(contracts_corr)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(contracts_corr, annot=True, cmap='coolwarm', center=0,\n",
    "                square=True, cbar_kws={'shrink': 0.8})\n",
    "    plt.title('Correlation Matrix - Contracts Numerical Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No numerical features found in contracts dataset\")\n",
    "\n",
    "# Cross-dataset analysis\n",
    "print(\"\\n=== CROSS-DATASET ANALYSIS ===\")\n",
    "\n",
    "# Merge datasets to find relationships\n",
    "if 'vulnerability_type' in bug_logs_df.columns and 'vulnerability_type' in contracts_df.columns:\n",
    "    # Create aggregated statistics per vulnerability type\n",
    "    bug_stats = bug_logs_df.groupby('vulnerability_type').agg({\n",
    "        'loc': ['count', 'mean', 'std'] if 'loc' in bug_logs_df.columns else 'count',\n",
    "        'length': ['mean', 'std'] if 'length' in bug_logs_df.columns else 'count'\n",
    "    }).reset_index()\n",
    "    \n",
    "    contract_stats = contracts_df.groupby('vulnerability_type').agg({\n",
    "        'line_count': ['mean', 'std'],\n",
    "        'char_count': ['mean', 'std'],\n",
    "        'file_size_kb': ['mean', 'std']\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten column names\n",
    "    bug_stats.columns = ['_'.join(col).strip() if col[1] else col[0] for col in bug_stats.columns.values]\n",
    "    contract_stats.columns = ['_'.join(col).strip() if col[1] else col[0] for col in contract_stats.columns.values]\n",
    "    \n",
    "    # Merge for correlation analysis\n",
    "    merged_stats = pd.merge(bug_stats, contract_stats, on='vulnerability_type', how='inner')\n",
    "    merged_numeric = merged_stats.select_dtypes(include=[np.number])\n",
    "    \n",
    "    if not merged_numeric.empty and len(merged_numeric.columns) > 1:\n",
    "        merged_corr = merged_numeric.corr()\n",
    "        print(\"Cross-dataset correlation:\")\n",
    "        print(merged_corr)\n",
    "        \n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(merged_corr, annot=True, cmap='coolwarm', center=0,\n",
    "                    square=True, cbar_kws={'shrink': 0.8})\n",
    "        plt.title('Cross-Dataset Correlation Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Insufficient numerical data for cross-dataset correlation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc478d4d",
   "metadata": {},
   "source": [
    "## 10. Feature Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd00216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distribution analysis and outlier detection\n",
    "print(\"=== FEATURE DISTRIBUTION ANALYSIS ===\")\n",
    "\n",
    "# Analyze outliers using IQR method\n",
    "def detect_outliers(df, column):\n",
    "    \"\"\"Detect outliers using IQR method\"\"\"\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Outlier analysis for numerical columns\n",
    "print(\"OUTLIER ANALYSIS:\")\n",
    "\n",
    "for dataset_name, dataset in [(\"Bug Logs\", bug_logs_df), (\"Contracts\", contracts_df)]:\n",
    "    print(f\"\\n{dataset_name} Dataset:\")\n",
    "    numeric_cols = dataset.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        outliers, lower, upper = detect_outliers(dataset, col)\n",
    "        outlier_pct = (len(outliers) / len(dataset)) * 100\n",
    "        print(f\"  {col}:\")\n",
    "        print(f\"    - Outliers: {len(outliers)} ({outlier_pct:.2f}%)\")\n",
    "        print(f\"    - Valid range: [{lower:.2f}, {upper:.2f}]\")\n",
    "\n",
    "# Distribution plots for key features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot distributions with outlier detection\n",
    "numeric_cols_contracts = contracts_df.select_dtypes(include=[np.number]).columns\n",
    "numeric_cols_bugs = bug_logs_df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "if len(numeric_cols_contracts) > 0:\n",
    "    col1 = numeric_cols_contracts[0]  # Usually line_count\n",
    "    axes[0, 0].hist(contracts_df[col1], bins=30, alpha=0.7, edgecolor='black')\n",
    "    axes[0, 0].axvline(contracts_df[col1].mean(), color='red', linestyle='--', label=f'Mean: {contracts_df[col1].mean():.2f}')\n",
    "    axes[0, 0].axvline(contracts_df[col1].median(), color='green', linestyle='--', label=f'Median: {contracts_df[col1].median():.2f}')\n",
    "    axes[0, 0].set_title(f'Distribution of {col1}')\n",
    "    axes[0, 0].legend()\n",
    "\n",
    "if len(numeric_cols_contracts) > 1:\n",
    "    col2 = numeric_cols_contracts[1]  # Usually char_count\n",
    "    axes[0, 1].hist(contracts_df[col2], bins=30, alpha=0.7, edgecolor='black', color='orange')\n",
    "    axes[0, 1].axvline(contracts_df[col2].mean(), color='red', linestyle='--', label=f'Mean: {contracts_df[col2].mean():.2f}')\n",
    "    axes[0, 1].axvline(contracts_df[col2].median(), color='green', linestyle='--', label=f'Median: {contracts_df[col2].median():.2f}')\n",
    "    axes[0, 1].set_title(f'Distribution of {col2}')\n",
    "    axes[0, 1].legend()\n",
    "\n",
    "if len(numeric_cols_bugs) > 0:\n",
    "    col3 = numeric_cols_bugs[0]  # Usually loc\n",
    "    axes[1, 0].hist(bug_logs_df[col3], bins=30, alpha=0.7, edgecolor='black', color='green')\n",
    "    axes[1, 0].axvline(bug_logs_df[col3].mean(), color='red', linestyle='--', label=f'Mean: {bug_logs_df[col3].mean():.2f}')\n",
    "    axes[1, 0].axvline(bug_logs_df[col3].median(), color='purple', linestyle='--', label=f'Median: {bug_logs_df[col3].median():.2f}')\n",
    "    axes[1, 0].set_title(f'Distribution of {col3}')\n",
    "    axes[1, 0].legend()\n",
    "\n",
    "if len(numeric_cols_bugs) > 1:\n",
    "    col4 = numeric_cols_bugs[1]  # Usually length\n",
    "    axes[1, 1].hist(bug_logs_df[col4], bins=30, alpha=0.7, edgecolor='black', color='purple')\n",
    "    axes[1, 1].axvline(bug_logs_df[col4].mean(), color='red', linestyle='--', label=f'Mean: {bug_logs_df[col4].mean():.2f}')\n",
    "    axes[1, 1].axvline(bug_logs_df[col4].median(), color='green', linestyle='--', label=f'Median: {bug_logs_df[col4].median():.2f}')\n",
    "    axes[1, 1].set_title(f'Distribution of {col4}')\n",
    "    axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Balance analysis for categorical variables\n",
    "print(\"\\n=== CATEGORICAL VARIABLE BALANCE ANALYSIS ===\")\n",
    "\n",
    "for dataset_name, dataset in [(\"Bug Logs\", bug_logs_df), (\"Contracts\", contracts_df)]:\n",
    "    print(f\"\\n{dataset_name} Dataset:\")\n",
    "    categorical_cols = dataset.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        value_counts = dataset[col].value_counts()\n",
    "        print(f\"\\n  {col} Balance:\")\n",
    "        for value, count in value_counts.items():\n",
    "            percentage = (count / len(dataset)) * 100\n",
    "            print(f\"    {value}: {count} ({percentage:.2f}%)\")\n",
    "        \n",
    "        # Calculate balance ratio (most common / least common)\n",
    "        if len(value_counts) > 1:\n",
    "            balance_ratio = value_counts.iloc[0] / value_counts.iloc[-1]\n",
    "            print(f\"    Balance ratio: {balance_ratio:.2f}:1\")\n",
    "\n",
    "print(\"\\n=== SUMMARY FOR ML PREPARATION ===\")\n",
    "print(\"Key findings for machine learning preparation:\")\n",
    "print(\"1. Dataset sizes and balance\")\n",
    "print(\"2. Outlier presence and treatment needs\")  \n",
    "print(\"3. Feature distributions (normal, skewed, etc.)\")\n",
    "print(\"4. Missing value patterns\")\n",
    "print(\"5. Categorical variable encoding requirements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b506347",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This comprehensive analysis of the smart contract vulnerability detection dataset reveals:\n",
    "\n",
    "### Key Findings:\n",
    "1. **Dataset Structure**: The dataset contains multiple vulnerability categories with corresponding Solidity contracts and bug logs\n",
    "2. **Vulnerability Types**: 7 main vulnerability categories are present in the dataset\n",
    "3. **Data Quality**: Analysis of missing values, outliers, and data consistency\n",
    "4. **Feature Characteristics**: Understanding of numerical and categorical feature distributions\n",
    "\n",
    "### Next Steps for ML Pipeline:\n",
    "1. **Data Preprocessing**: Handle outliers, normalize features, encode categorical variables\n",
    "2. **Feature Engineering**: Extract additional features from smart contract code\n",
    "3. **Model Selection**: Choose appropriate algorithms for vulnerability classification\n",
    "4. **Evaluation**: Implement proper validation strategies for imbalanced datasets\n",
    "\n",
    "This analysis provides a solid foundation for developing machine learning models for smart contract vulnerability detection."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
